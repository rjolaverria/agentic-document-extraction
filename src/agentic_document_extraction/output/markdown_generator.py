"""Markdown output generator for extraction results.

This module provides functionality to generate formatted Markdown summaries
from extraction results, including source references and confidence indicators.
"""

import logging
from dataclasses import dataclass, field
from typing import Any

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from agentic_document_extraction.config import settings
from agentic_document_extraction.services.extraction.text_extraction import (
    ExtractionResult,
    FieldExtraction,
)
from agentic_document_extraction.services.schema_validator import SchemaInfo

logger = logging.getLogger(__name__)


@dataclass
class MarkdownOutputResult:
    """Result of Markdown generation."""

    markdown: str
    """The generated Markdown content."""

    source_references: list[dict[str, Any]] = field(default_factory=list)
    """List of source references included in the Markdown."""

    generated_by_llm: bool = False
    """Whether the Markdown was generated by an LLM."""

    token_usage: dict[str, int] = field(default_factory=dict)
    """Token usage if LLM was used."""

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation.

        Returns:
            Dictionary with Markdown output result information.
        """
        return {
            "markdown": self.markdown,
            "source_references": self.source_references,
            "generated_by_llm": self.generated_by_llm,
            "token_usage": self.token_usage,
        }


class MarkdownGenerator:
    """Generates formatted Markdown summaries from extraction results.

    Can generate Markdown either programmatically or using LangChain
    with an LLM for more natural formatting.
    """

    MARKDOWN_SYSTEM_PROMPT = """You are an expert at creating clear, well-formatted Markdown summaries.
Your task is to create a Markdown summary of extracted information.

Guidelines:
1. Use clear headings and subheadings
2. Format data appropriately (tables for structured data, lists for items)
3. Include confidence indicators when available
4. Highlight any missing or uncertain information
5. Be concise but comprehensive
6. Use proper Markdown formatting (bold, italic, code blocks, etc.)"""

    MARKDOWN_USER_PROMPT = """Create a Markdown summary of the following extracted data.

## Schema Description:
{schema_description}

## Extracted Data:
{extracted_data}

## Field Details (with confidence and source):
{field_details}

Generate a well-formatted Markdown summary that presents this information clearly.
Include source references where available."""

    def __init__(
        self,
        use_llm: bool = True,
        api_key: str | None = None,
        model: str | None = None,
    ) -> None:
        """Initialize the Markdown generator.

        Args:
            use_llm: Whether to use LLM for generating Markdown.
            api_key: OpenAI API key. Defaults to settings.
            model: Model to use. Defaults to settings.
        """
        self.use_llm = use_llm
        self.api_key = api_key if api_key is not None else settings.get_openai_api_key()
        self.model = model or settings.openai_model
        self._llm: ChatOpenAI | None = None

    @property
    def llm(self) -> ChatOpenAI | None:
        """Get or create the LangChain ChatOpenAI instance.

        Returns:
            Configured ChatOpenAI instance or None if not available.
        """
        if not self.use_llm or not self.api_key:
            return None

        if self._llm is None:
            self._llm = ChatOpenAI(
                api_key=self.api_key,  # type: ignore[arg-type]
                model=self.model,
                temperature=0.3,  # Slightly creative for better formatting
                max_completion_tokens=2000,
            )

        return self._llm

    def generate(
        self,
        extraction_result: ExtractionResult,
        schema_info: SchemaInfo,
        include_source_refs: bool = True,
        include_confidence: bool = True,
    ) -> MarkdownOutputResult:
        """Generate Markdown summary from extraction result.

        Args:
            extraction_result: The extraction result to summarize.
            schema_info: Schema information for context.
            include_source_refs: Whether to include source references.
            include_confidence: Whether to include confidence indicators.

        Returns:
            MarkdownOutputResult with generated Markdown.
        """
        # Collect source references
        source_refs = self._collect_source_references(
            extraction_result.field_extractions
        )

        # Try LLM-based generation first
        if self.llm is not None:
            try:
                result = self._generate_with_llm(
                    extraction_result,
                    schema_info,
                    include_source_refs,
                    include_confidence,
                )
                result.source_references = source_refs
                return result
            except Exception as e:
                logger.warning(
                    f"LLM Markdown generation failed: {e}, falling back to template"
                )

        # Fall back to template-based generation
        markdown = self._generate_template_markdown(
            extraction_result,
            schema_info,
            include_source_refs,
            include_confidence,
        )

        return MarkdownOutputResult(
            markdown=markdown,
            source_references=source_refs,
            generated_by_llm=False,
        )

    def _generate_with_llm(
        self,
        extraction_result: ExtractionResult,
        schema_info: SchemaInfo,
        include_source_refs: bool,
        include_confidence: bool,
    ) -> MarkdownOutputResult:
        """Generate Markdown using LLM.

        Args:
            extraction_result: The extraction result.
            schema_info: Schema information.
            include_source_refs: Whether to include source references.
            include_confidence: Whether to include confidence indicators.

        Returns:
            MarkdownOutputResult with LLM-generated Markdown.
        """
        if self.llm is None:
            raise ValueError("LLM not available")

        # Build schema description
        schema_description = self._build_schema_description(schema_info)

        # Build field details
        field_details = self._build_field_details(
            extraction_result.field_extractions,
            include_source_refs,
            include_confidence,
        )

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", self.MARKDOWN_SYSTEM_PROMPT),
                ("human", self.MARKDOWN_USER_PROMPT),
            ]
        )

        messages = prompt.format_messages(
            schema_description=schema_description,
            extracted_data=self._format_data_for_prompt(
                extraction_result.extracted_data
            ),
            field_details=field_details,
        )

        response = self.llm.invoke(messages)

        content = response.content
        if not isinstance(content, str):
            content = str(content)

        # Extract token usage
        usage_metadata = getattr(response, "usage_metadata", None) or {}
        token_usage = {
            "prompt_tokens": usage_metadata.get("input_tokens", 0),
            "completion_tokens": usage_metadata.get("output_tokens", 0),
            "total_tokens": usage_metadata.get("total_tokens", 0),
        }

        return MarkdownOutputResult(
            markdown=content,
            generated_by_llm=True,
            token_usage=token_usage,
        )

    def _generate_template_markdown(
        self,
        extraction_result: ExtractionResult,
        schema_info: SchemaInfo,
        include_source_refs: bool,
        include_confidence: bool,
    ) -> str:
        """Generate Markdown using templates.

        Args:
            extraction_result: The extraction result.
            schema_info: Schema information.
            include_source_refs: Whether to include source references.
            include_confidence: Whether to include confidence indicators.

        Returns:
            Generated Markdown string.
        """
        lines: list[str] = []

        # Title
        lines.append("# Extraction Summary")
        lines.append("")

        # Overview section
        lines.append("## Overview")
        lines.append("")
        lines.append(f"- **Model Used:** {extraction_result.model_used}")
        lines.append(
            f"- **Processing Time:** {extraction_result.processing_time_seconds:.2f}s"
        )
        lines.append(f"- **Total Tokens:** {extraction_result.total_tokens}")
        if extraction_result.is_chunked:
            lines.append(
                f"- **Chunks Processed:** {extraction_result.chunks_processed}"
            )
        lines.append("")

        # Extracted Data section
        lines.append("## Extracted Data")
        lines.append("")

        self._render_data_section(
            lines,
            extraction_result.extracted_data,
            extraction_result.field_extractions,
            schema_info,
            include_confidence,
        )

        # Source References section
        if include_source_refs:
            source_refs = self._collect_source_references(
                extraction_result.field_extractions
            )
            if source_refs:
                lines.append("")
                lines.append("## Source References")
                lines.append("")
                for i, ref in enumerate(source_refs, 1):
                    lines.append(
                        f'{i}. **{ref["field"]}**: "{ref["source_text"][:100]}..."'
                    )

        # Metadata section
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append("*Generated by Agentic Document Extraction*")

        return "\n".join(lines)

    def _render_data_section(
        self,
        lines: list[str],
        data: dict[str, Any],
        field_extractions: list[FieldExtraction],
        schema_info: SchemaInfo,
        include_confidence: bool,
        prefix: str = "",
        depth: int = 0,
    ) -> None:
        """Render data section recursively.

        Args:
            lines: List of lines to append to.
            data: Data to render.
            field_extractions: Field extraction details.
            schema_info: Schema information.
            include_confidence: Whether to include confidence.
            prefix: Path prefix for nested fields.
            depth: Current nesting depth.
        """
        indent = "  " * depth

        for key, value in data.items():
            field_path = f"{prefix}.{key}" if prefix else key

            # Find field extraction for confidence
            field_ext = next(
                (fe for fe in field_extractions if fe.field_path == field_path),
                None,
            )

            # Note: field_info can be used for additional description if needed
            # field_info = next(
            #     (f for f in schema_info.all_fields if f.path == field_path),
            #     None,
            # )

            if isinstance(value, dict):
                lines.append(f"{indent}### {key}")
                lines.append("")
                self._render_data_section(
                    lines,
                    value,
                    field_extractions,
                    schema_info,
                    include_confidence,
                    field_path,
                    depth + 1,
                )
            elif isinstance(value, list):
                lines.append(f"{indent}- **{key}:**")
                for item in value:
                    if isinstance(item, dict):
                        lines.append(f"{indent}  - {self._format_dict_inline(item)}")
                    else:
                        lines.append(f"{indent}  - {item}")
            else:
                confidence_str = ""
                if (
                    include_confidence
                    and field_ext
                    and field_ext.confidence is not None
                ):
                    confidence_str = f" *(confidence: {field_ext.confidence:.0%})*"

                value_str = str(value) if value is not None else "*Not found*"
                lines.append(f"{indent}- **{key}:** {value_str}{confidence_str}")

    def _format_dict_inline(self, data: dict[str, Any]) -> str:
        """Format a dictionary inline for list items.

        Args:
            data: Dictionary to format.

        Returns:
            Inline formatted string.
        """
        parts = [f"{k}: {v}" for k, v in data.items() if v is not None]
        return ", ".join(parts)

    def _build_schema_description(self, schema_info: SchemaInfo) -> str:
        """Build a description of the schema.

        Args:
            schema_info: Schema information.

        Returns:
            Schema description string.
        """
        lines: list[str] = []
        lines.append(f"Schema type: {schema_info.schema_type}")
        lines.append("")

        if schema_info.required_fields:
            lines.append("Required fields:")
            for field_info in schema_info.required_fields:
                desc = f" - {field_info.description}" if field_info.description else ""
                lines.append(f"- {field_info.path} ({field_info.field_type}){desc}")

        if schema_info.optional_fields:
            lines.append("")
            lines.append("Optional fields:")
            for field_info in schema_info.optional_fields:
                desc = f" - {field_info.description}" if field_info.description else ""
                lines.append(f"- {field_info.path} ({field_info.field_type}){desc}")

        return "\n".join(lines)

    def _build_field_details(
        self,
        field_extractions: list[FieldExtraction],
        include_source_refs: bool,
        include_confidence: bool,
    ) -> str:
        """Build field details string.

        Args:
            field_extractions: List of field extractions.
            include_source_refs: Whether to include source references.
            include_confidence: Whether to include confidence.

        Returns:
            Field details string.
        """
        if not field_extractions:
            return "No detailed field information available."

        lines: list[str] = []

        for fe in field_extractions:
            parts = [f"- {fe.field_path}: {fe.value}"]

            if include_confidence and fe.confidence is not None:
                parts.append(f" (confidence: {fe.confidence:.0%})")

            if include_source_refs and fe.source_text:
                parts.append(f'\n  Source: "{fe.source_text[:100]}..."')

            if fe.reasoning:
                parts.append(f"\n  Reasoning: {fe.reasoning}")

            lines.append("".join(parts))

        return "\n".join(lines)

    def _format_data_for_prompt(self, data: dict[str, Any]) -> str:
        """Format data for inclusion in prompt.

        Args:
            data: Data to format.

        Returns:
            Formatted string.
        """
        import json

        return json.dumps(data, indent=2, default=str)

    def _collect_source_references(
        self,
        field_extractions: list[FieldExtraction],
    ) -> list[dict[str, Any]]:
        """Collect source references from field extractions.

        Args:
            field_extractions: List of field extractions.

        Returns:
            List of source reference dictionaries.
        """
        refs: list[dict[str, Any]] = []

        for fe in field_extractions:
            if fe.source_text:
                refs.append(
                    {
                        "field": fe.field_path,
                        "source_text": fe.source_text,
                        "confidence": fe.confidence,
                    }
                )

        return refs
